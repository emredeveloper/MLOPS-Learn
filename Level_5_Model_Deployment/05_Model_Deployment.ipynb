{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ğŸ“¦ Level 5: Model Deployment - Production'a GeÃ§iÅŸ\n",
        "\n",
        "## ğŸ¯ Bu BÃ¶lÃ¼mde Neler Ã–ÄŸreneceÄŸiz?\n",
        "\n",
        "1. **Model Packaging**: Modelleri production iÃ§in hazÄ±rlama\n",
        "2. **REST API Development**: Flask/FastAPI ile model servisi\n",
        "3. **Docker Containerization**: Modelleri containerize etme\n",
        "4. **Cloud Deployment**: AWS/Azure/GCP'ye deploy etme\n",
        "5. **Load Testing**: Model performansÄ±nÄ± test etme\n",
        "6. **CI/CD Pipeline**: Otomatik deployment sÃ¼reÃ§leri\n",
        "\n",
        "### ğŸ—ï¸ Production Deployment Architecture\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   Data Input    â”‚â”€â”€â”€â–¶â”‚  Model Service  â”‚â”€â”€â”€â–¶â”‚   Predictions   â”‚\n",
        "â”‚   (REST API)    â”‚    â”‚   (Container)   â”‚    â”‚   (JSON/DB)     â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "         â”‚                       â”‚                       â”‚\n",
        "         â–¼                       â–¼                       â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   Monitoring    â”‚    â”‚   Load Balancer â”‚    â”‚     Logging     â”‚\n",
        "â”‚   (Metrics)     â”‚    â”‚   (Auto-scale)  â”‚    â”‚   (Analytics)   â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¦ TÃ¼m kÃ¼tÃ¼phaneler baÅŸarÄ±yla yÃ¼klendi!\n",
            "ğŸš€ Model Deployment eÄŸitimine hazÄ±rÄ±z!\n",
            "ğŸ“… BaÅŸlangÄ±Ã§ zamanÄ±: 2025-07-02 17:21:39\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“¦ Gerekli kÃ¼tÃ¼phaneleri yÃ¼kleyelim\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import joblib\n",
        "import requests\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ML libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# MLflow\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Web framework\n",
        "from flask import Flask, request, jsonify\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import threading\n",
        "\n",
        "# Testing ve monitoring\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import psutil\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Plotting ayarlarÄ±\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"ğŸ“¦ TÃ¼m kÃ¼tÃ¼phaneler baÅŸarÄ±yla yÃ¼klendi!\")\n",
        "print(\"ğŸš€ Model Deployment eÄŸitimine hazÄ±rÄ±z!\")\n",
        "print(f\"ğŸ“… BaÅŸlangÄ±Ã§ zamanÄ±: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”§ DÃ¼zeltilmiÅŸ Model Packaging Kodu\n",
        "class ModelPackager:\n",
        "    \"\"\"Model packaging ve versioning iÃ§in sÄ±nÄ±f\"\"\"\n",
        "    \n",
        "    def __init__(self, model_dir=\"models\"):\n",
        "        self.model_dir = Path(model_dir)\n",
        "        self.model_dir.mkdir(exist_ok=True)\n",
        "        self.metadata = {}\n",
        "    \n",
        "    def prepare_model(self):\n",
        "        \"\"\"Demo iÃ§in basit bir model hazÄ±rla\"\"\"\n",
        "        print(\"ğŸ  California Housing veri seti ile model eÄŸitiliyor...\")\n",
        "        \n",
        "        # Veri yÃ¼kleme\n",
        "        california_housing = fetch_california_housing()\n",
        "        X = pd.DataFrame(california_housing.data, columns=california_housing.feature_names)\n",
        "        y = california_housing.target\n",
        "        \n",
        "        # Train-test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "        \n",
        "        # Preprocessing\n",
        "        self.scaler = StandardScaler()\n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "        \n",
        "        # Model eÄŸitimi\n",
        "        self.model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        self.model.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        # Test performansÄ±\n",
        "        y_pred = self.model.predict(X_test_scaled)\n",
        "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        test_r2 = r2_score(y_test, y_pred)\n",
        "        \n",
        "        # Metadata (dÃ¼zeltilmiÅŸ - .tolist() hatasÄ± giderildi)\n",
        "        self.metadata = {\n",
        "            'model_type': 'RandomForestRegressor',\n",
        "            'version': '1.0.0',\n",
        "            'training_date': datetime.now().isoformat(),\n",
        "            'test_rmse': test_rmse,\n",
        "            'test_r2': test_r2,\n",
        "            'feature_names': list(california_housing.feature_names),  # DÃ¼zeltme burada\n",
        "            'input_shape': X.shape[1],\n",
        "            'target_name': 'house_value'\n",
        "        }\n",
        "        \n",
        "        print(f\"âœ… Model eÄŸitildi!\")\n",
        "        print(f\"ğŸ“Š Test RMSE: {test_rmse:.4f}\")\n",
        "        print(f\"ğŸ“Š Test RÂ²: {test_r2:.4f}\")\n",
        "        \n",
        "        return X_test, y_test\n",
        "    \n",
        "    def save_model(self, version=\"v1\"):\n",
        "        \"\"\"Modeli ve preprocessing pipeline'Ä±nÄ± kaydet\"\"\"\n",
        "        version_dir = self.model_dir / version\n",
        "        version_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        # Model kaydet\n",
        "        model_path = version_dir / \"model.pkl\"\n",
        "        joblib.dump(self.model, model_path)\n",
        "        \n",
        "        # Scaler kaydet\n",
        "        scaler_path = version_dir / \"scaler.pkl\"\n",
        "        joblib.dump(self.scaler, scaler_path)\n",
        "        \n",
        "        # Metadata kaydet\n",
        "        metadata_path = version_dir / \"metadata.json\"\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(self.metadata, f, indent=2)\n",
        "        \n",
        "        print(f\"ğŸ’¾ Model kaydedildi: {version_dir}\")\n",
        "        print(f\"   ğŸ“ Model: {model_path}\")\n",
        "        print(f\"   ğŸ“ Scaler: {scaler_path}\")\n",
        "        print(f\"   ğŸ“ Metadata: {metadata_path}\")\n",
        "        \n",
        "        return version_dir\n",
        "    \n",
        "    def load_model(self, version=\"v1\"):\n",
        "        \"\"\"KaydedilmiÅŸ modeli yÃ¼kle\"\"\"\n",
        "        version_dir = self.model_dir / version\n",
        "        \n",
        "        if not version_dir.exists():\n",
        "            raise FileNotFoundError(f\"Model version {version} bulunamadÄ±!\")\n",
        "        \n",
        "        # Model yÃ¼kle\n",
        "        model_path = version_dir / \"model.pkl\"\n",
        "        self.model = joblib.load(model_path)\n",
        "        \n",
        "        # Scaler yÃ¼kle\n",
        "        scaler_path = version_dir / \"scaler.pkl\"\n",
        "        self.scaler = joblib.load(scaler_path)\n",
        "        \n",
        "        # Metadata yÃ¼kle\n",
        "        metadata_path = version_dir / \"metadata.json\"\n",
        "        with open(metadata_path, 'r') as f:\n",
        "            self.metadata = json.load(f)\n",
        "        \n",
        "        print(f\"ğŸ“¥ Model yÃ¼klendi: {version}\")\n",
        "        print(f\"ğŸ“Š Model Type: {self.metadata['model_type']}\")\n",
        "        print(f\"ğŸ“Š Version: {self.metadata['version']}\")\n",
        "        print(f\"ğŸ“Š Test RÂ²: {self.metadata['test_r2']:.4f}\")\n",
        "        \n",
        "        return self.model, self.scaler, self.metadata\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Tahmin yap\"\"\"\n",
        "        if hasattr(X, 'values'):\n",
        "            X = X.values\n",
        "        \n",
        "        X_scaled = self.scaler.transform(X)\n",
        "        predictions = self.model.predict(X_scaled)\n",
        "        \n",
        "        return predictions\n",
        "\n",
        "# Model paketi oluÅŸtur (dÃ¼zeltilmiÅŸ sÃ¼rÃ¼m)\n",
        "packager = ModelPackager()\n",
        "X_test, y_test = packager.prepare_model()\n",
        "model_version_dir = packager.save_model(\"v1\")\n",
        "\n",
        "print(\"\\nâœ… Model packaging tamamlandÄ±!\")\n",
        "print(f\"ğŸ“ Model dizini: {model_version_dir}\")\n",
        "print(f\"ğŸ“Š Feature adlarÄ±: {packager.metadata['feature_names']}\")\n",
        "print(f\"ğŸ“Š Input shape: {packager.metadata['input_shape']}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Model HazÄ±rlama ve Packaging\n",
        "\n",
        "Ä°lk olarak Level 4'ten en iyi modeli alÄ±p deployment iÃ§in hazÄ±rlayacaÄŸÄ±z:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ  California Housing veri seti ile model eÄŸitiliyor...\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'tolist'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 119\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# Model paketi oluÅŸtur\u001b[39;00m\n\u001b[32m    118\u001b[39m packager = ModelPackager()\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m X_test, y_test = \u001b[43mpackager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m model_version_dir = packager.save_model(\u001b[33m\"\u001b[39m\u001b[33mv1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    122\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ… Model packaging tamamlandÄ±!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mModelPackager.prepare_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     35\u001b[39m test_r2 = r2_score(y_test, y_pred)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Metadata\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mself\u001b[39m.metadata = {\n\u001b[32m     39\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mRandomForestRegressor\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     40\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33m1.0.0\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     41\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtraining_date\u001b[39m\u001b[33m'\u001b[39m: datetime.now().isoformat(),\n\u001b[32m     42\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtest_rmse\u001b[39m\u001b[33m'\u001b[39m: test_rmse,\n\u001b[32m     43\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtest_r2\u001b[39m\u001b[33m'\u001b[39m: test_r2,\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfeature_names\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mcalifornia_housing\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m(),\n\u001b[32m     45\u001b[39m     \u001b[33m'\u001b[39m\u001b[33minput_shape\u001b[39m\u001b[33m'\u001b[39m: X.shape[\u001b[32m1\u001b[39m],\n\u001b[32m     46\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtarget_name\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mhouse_value\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     47\u001b[39m }\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Model eÄŸitildi!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mğŸ“Š Test RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'tolist'"
          ]
        }
      ],
      "source": [
        "class ModelPackager:\n",
        "    \"\"\"Model packaging ve versioning iÃ§in sÄ±nÄ±f\"\"\"\n",
        "    \n",
        "    def __init__(self, model_dir=\"models\"):\n",
        "        self.model_dir = Path(model_dir)\n",
        "        self.model_dir.mkdir(exist_ok=True)\n",
        "        self.metadata = {}\n",
        "    \n",
        "    def prepare_model(self):\n",
        "        \"\"\"Demo iÃ§in basit bir model hazÄ±rla\"\"\"\n",
        "        print(\"ğŸ  California Housing veri seti ile model eÄŸitiliyor...\")\n",
        "        \n",
        "        # Veri yÃ¼kleme\n",
        "        california_housing = fetch_california_housing()\n",
        "        X = pd.DataFrame(california_housing.data, columns=california_housing.feature_names)\n",
        "        y = california_housing.target\n",
        "        \n",
        "        # Train-test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "        \n",
        "        # Preprocessing\n",
        "        self.scaler = StandardScaler()\n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "        \n",
        "        # Model eÄŸitimi\n",
        "        self.model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        self.model.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        # Test performansÄ±\n",
        "        y_pred = self.model.predict(X_test_scaled)\n",
        "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        test_r2 = r2_score(y_test, y_pred)\n",
        "        \n",
        "        # Metadata\n",
        "        self.metadata = {\n",
        "            'model_type': 'RandomForestRegressor',\n",
        "            'version': '1.0.0',\n",
        "            'training_date': datetime.now().isoformat(),\n",
        "            'test_rmse': test_rmse,\n",
        "            'test_r2': test_r2,\n",
        "            'feature_names': california_housing.feature_names.tolist(),\n",
        "            'input_shape': X.shape[1],\n",
        "            'target_name': 'house_value'\n",
        "        }\n",
        "        \n",
        "        print(f\"âœ… Model eÄŸitildi!\")\n",
        "        print(f\"ğŸ“Š Test RMSE: {test_rmse:.4f}\")\n",
        "        print(f\"ğŸ“Š Test RÂ²: {test_r2:.4f}\")\n",
        "        \n",
        "        return X_test, y_test\n",
        "    \n",
        "    def save_model(self, version=\"v1\"):\n",
        "        \"\"\"Modeli ve preprocessing pipeline'Ä±nÄ± kaydet\"\"\"\n",
        "        version_dir = self.model_dir / version\n",
        "        version_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        # Model kaydet\n",
        "        model_path = version_dir / \"model.pkl\"\n",
        "        joblib.dump(self.model, model_path)\n",
        "        \n",
        "        # Scaler kaydet\n",
        "        scaler_path = version_dir / \"scaler.pkl\"\n",
        "        joblib.dump(self.scaler, scaler_path)\n",
        "        \n",
        "        # Metadata kaydet\n",
        "        metadata_path = version_dir / \"metadata.json\"\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(self.metadata, f, indent=2)\n",
        "        \n",
        "        print(f\"ğŸ’¾ Model kaydedildi: {version_dir}\")\n",
        "        print(f\"   ğŸ“ Model: {model_path}\")\n",
        "        print(f\"   ğŸ“ Scaler: {scaler_path}\")\n",
        "        print(f\"   ğŸ“ Metadata: {metadata_path}\")\n",
        "        \n",
        "        return version_dir\n",
        "    \n",
        "    def load_model(self, version=\"v1\"):\n",
        "        \"\"\"KaydedilmiÅŸ modeli yÃ¼kle\"\"\"\n",
        "        version_dir = self.model_dir / version\n",
        "        \n",
        "        if not version_dir.exists():\n",
        "            raise FileNotFoundError(f\"Model version {version} bulunamadÄ±!\")\n",
        "        \n",
        "        # Model yÃ¼kle\n",
        "        model_path = version_dir / \"model.pkl\"\n",
        "        self.model = joblib.load(model_path)\n",
        "        \n",
        "        # Scaler yÃ¼kle\n",
        "        scaler_path = version_dir / \"scaler.pkl\"\n",
        "        self.scaler = joblib.load(scaler_path)\n",
        "        \n",
        "        # Metadata yÃ¼kle\n",
        "        metadata_path = version_dir / \"metadata.json\"\n",
        "        with open(metadata_path, 'r') as f:\n",
        "            self.metadata = json.load(f)\n",
        "        \n",
        "        print(f\"ğŸ“¥ Model yÃ¼klendi: {version}\")\n",
        "        print(f\"ğŸ“Š Model Type: {self.metadata['model_type']}\")\n",
        "        print(f\"ğŸ“Š Version: {self.metadata['version']}\")\n",
        "        print(f\"ğŸ“Š Test RÂ²: {self.metadata['test_r2']:.4f}\")\n",
        "        \n",
        "        return self.model, self.scaler, self.metadata\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Tahmin yap\"\"\"\n",
        "        if hasattr(X, 'values'):\n",
        "            X = X.values\n",
        "        \n",
        "        X_scaled = self.scaler.transform(X)\n",
        "        predictions = self.model.predict(X_scaled)\n",
        "        \n",
        "        return predictions\n",
        "\n",
        "# Model paketi oluÅŸtur\n",
        "packager = ModelPackager()\n",
        "X_test, y_test = packager.prepare_model()\n",
        "model_version_dir = packager.save_model(\"v1\")\n",
        "\n",
        "print(\"\\nâœ… Model packaging tamamlandÄ±!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 6. Ã–zet ve SonuÃ§lar\n",
        "\n",
        "### ğŸ¯ Bu BÃ¶lÃ¼mde Neler Ã–ÄŸrendik?\n",
        "\n",
        "1. **Model Packaging**: Modelleri production-ready hale getirme\n",
        "2. **REST API Development**: FastAPI ile profesyonel API geliÅŸtirme\n",
        "3. **API Testing**: KapsamlÄ± test stratejileri\n",
        "4. **Docker Containerization**: Modelleri containerize etme\n",
        "5. **Load Testing**: Performance ve scalability analizi\n",
        "6. **Monitoring**: System health ve performance tracking\n",
        "\n",
        "### ğŸ“Š Deployment Best Practices:\n",
        "\n",
        "- **Model Versioning**: Her model versiyonunu ayrÄ± dizinlerde saklama\n",
        "- **Error Handling**: Comprehensive hata yÃ¶netimi\n",
        "- **Health Checks**: System durumu monitoring\n",
        "- **Load Testing**: Production Ã¶ncesi performance testi\n",
        "- **Documentation**: API documentation (FastAPI Swagger)\n",
        "- **Containerization**: Docker ile portable deployment\n",
        "\n",
        "### ğŸš€ Production Deployment Checklist:\n",
        "\n",
        "#### âœ… Model HazÄ±rlÄ±ÄŸÄ±\n",
        "- [ ] Model eÄŸitildi ve test edildi\n",
        "- [ ] Preprocessing pipeline kaydedildi\n",
        "- [ ] Model metadata oluÅŸturuldu\n",
        "- [ ] Model versioning sistemi kuruldu\n",
        "\n",
        "#### âœ… API Development\n",
        "- [ ] FastAPI application oluÅŸturuldu\n",
        "- [ ] Input validation implement edildi\n",
        "- [ ] Error handling eklendi\n",
        "- [ ] Health check endpoint'i eklendi\n",
        "- [ ] API documentation hazÄ±rlandÄ±\n",
        "\n",
        "#### âœ… Testing\n",
        "- [ ] Unit testler yazÄ±ldÄ±\n",
        "- [ ] Integration testler yapÄ±ldÄ±\n",
        "- [ ] Load testing tamamlandÄ±\n",
        "- [ ] Error scenarios test edildi\n",
        "\n",
        "#### âœ… Deployment\n",
        "- [ ] Dockerfile oluÅŸturuldu\n",
        "- [ ] Docker Compose konfigÃ¼rasyonu hazÄ±rlandÄ±\n",
        "- [ ] Environment variables tanÄ±mlandÄ±\n",
        "- [ ] Health checks konfigÃ¼re edildi\n",
        "\n",
        "#### âœ… Monitoring\n",
        "- [ ] Performance metrics toplanÄ±yor\n",
        "- [ ] Error logging aktif\n",
        "- [ ] Health monitoring kuruldu\n",
        "- [ ] Alerting sistemi aktif\n",
        "\n",
        "### ğŸ’¡ Advanced Deployment Strategies:\n",
        "\n",
        "1. **Blue-Green Deployment**: Zero-downtime deployment\n",
        "2. **Canary Deployment**: Gradual rollout\n",
        "3. **A/B Testing**: Model comparison in production\n",
        "4. **Auto-scaling**: Traffic'e gÃ¶re scaling\n",
        "5. **Circuit Breaker**: Error durumunda koruma\n",
        "6. **Rate Limiting**: API abuse korumasÄ±\n",
        "\n",
        "### ğŸ”§ Production Tools:\n",
        "\n",
        "- **Container Orchestration**: Kubernetes, Docker Swarm\n",
        "- **API Gateway**: Kong, Ambassador, AWS API Gateway\n",
        "- **Monitoring**: Prometheus, Grafana, ELK Stack\n",
        "- **CI/CD**: Jenkins, GitLab CI, GitHub Actions\n",
        "- **Cloud Platforms**: AWS, Azure, GCP\n",
        "- **Load Balancing**: Nginx, HAProxy, AWS ALB\n",
        "\n",
        "### ğŸ“ˆ Sonraki AdÄ±mlar:\n",
        "\n",
        "1. **Level 6 - Monitoring**: Model performance monitoring\n",
        "2. **A/B Testing**: FarklÄ± model versiyonlarÄ±nÄ± test etme\n",
        "3. **Automated Retraining**: Data drift detection ve otomatik model gÃ¼ncelleme\n",
        "4. **Multi-model Serving**: Birden fazla modeli aynÄ± API'de servis etme\n",
        "\n",
        "---\n",
        "\n",
        "**Level 5 TamamlandÄ±! ğŸ‰**\n",
        "\n",
        "ArtÄ±k ML modellerinizi production ortamÄ±nda gÃ¼venle deploy edebilir, test edebilir ve monitor edebilirsiniz!\n",
        "\n",
        "### ğŸ“š Ek Kaynaklar:\n",
        "\n",
        "- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n",
        "- [Docker Best Practices](https://docs.docker.com/develop/best-practices/)\n",
        "- [MLOps Deployment Patterns](https://ml-ops.org/content/deployment-patterns)\n",
        "- [Model Serving Strategies](https://developers.google.com/machine-learning/guides/rules-of-ml)\n",
        "\n",
        "**ğŸš€ Production'a hazÄ±r ML servisi oluÅŸturdunuz!**\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. REST API Development\n",
        "\n",
        "FastAPI kullanarak model servisi oluÅŸturacaÄŸÄ±z:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModelService:\n",
        "    \"\"\"Model servisi iÃ§in sÄ±nÄ±f\"\"\"\n",
        "    \n",
        "    def __init__(self, model_version=\"v1\"):\n",
        "        self.packager = ModelPackager()\n",
        "        self.model, self.scaler, self.metadata = self.packager.load_model(model_version)\n",
        "        self.prediction_count = 0\n",
        "        self.start_time = time.time()\n",
        "    \n",
        "    def predict_single(self, features):\n",
        "        \"\"\"Tek tahmin yap\"\"\"\n",
        "        try:\n",
        "            # Feature validation\n",
        "            if len(features) != self.metadata['input_shape']:\n",
        "                raise ValueError(f\"Expected {self.metadata['input_shape']} features, got {len(features)}\")\n",
        "            \n",
        "            # Prediction\n",
        "            X = np.array(features).reshape(1, -1)\n",
        "            prediction = self.packager.predict(X)[0]\n",
        "            \n",
        "            self.prediction_count += 1\n",
        "            \n",
        "            return {\n",
        "                'prediction': float(prediction),\n",
        "                'model_version': self.metadata['version'],\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'prediction_id': self.prediction_count\n",
        "            }\n",
        "        \n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'error': str(e),\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "    \n",
        "    def predict_batch(self, features_list):\n",
        "        \"\"\"Batch tahmin yap\"\"\"\n",
        "        try:\n",
        "            predictions = []\n",
        "            for features in features_list:\n",
        "                result = self.predict_single(features)\n",
        "                predictions.append(result)\n",
        "            \n",
        "            return {\n",
        "                'predictions': predictions,\n",
        "                'batch_size': len(features_list),\n",
        "                'model_version': self.metadata['version'],\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "        \n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'error': str(e),\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "    \n",
        "    def get_health(self):\n",
        "        \"\"\"Health check\"\"\"\n",
        "        uptime = time.time() - self.start_time\n",
        "        \n",
        "        return {\n",
        "            'status': 'healthy',\n",
        "            'uptime_seconds': uptime,\n",
        "            'total_predictions': self.prediction_count,\n",
        "            'model_version': self.metadata['version'],\n",
        "            'model_type': self.metadata['model_type'],\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "    \n",
        "    def get_model_info(self):\n",
        "        \"\"\"Model bilgilerini dÃ¶ndÃ¼r\"\"\"\n",
        "        return {\n",
        "            'metadata': self.metadata,\n",
        "            'service_stats': {\n",
        "                'uptime_seconds': time.time() - self.start_time,\n",
        "                'total_predictions': self.prediction_count\n",
        "            }\n",
        "        }\n",
        "\n",
        "# Model servisini baÅŸlat\n",
        "model_service = ModelService(\"v1\")\n",
        "\n",
        "print(\"ğŸš€ Model servisi hazÄ±r!\")\n",
        "print(f\"ğŸ“Š Model: {model_service.metadata['model_type']}\")\n",
        "print(f\"ğŸ“Š Version: {model_service.metadata['version']}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### FastAPI Application OluÅŸturma\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pydantic modelleri\n",
        "class PredictionRequest(BaseModel):\n",
        "    features: list\n",
        "    \n",
        "class BatchPredictionRequest(BaseModel):\n",
        "    features_list: list\n",
        "\n",
        "# FastAPI app oluÅŸtur\n",
        "def create_fastapi_app():\n",
        "    app = FastAPI(\n",
        "        title=\"ğŸ  California Housing Price Predictor\",\n",
        "        description=\"Machine Learning model service for house price prediction\",\n",
        "        version=\"1.0.0\"\n",
        "    )\n",
        "    \n",
        "    # Global model service\n",
        "    service = ModelService(\"v1\")\n",
        "    \n",
        "    @app.get(\"/\")\n",
        "    async def root():\n",
        "        return {\n",
        "            \"message\": \"ğŸ  California Housing Price Predictor API\",\n",
        "            \"version\": \"1.0.0\",\n",
        "            \"model_version\": service.metadata['version'],\n",
        "            \"docs\": \"/docs\"\n",
        "        }\n",
        "    \n",
        "    @app.get(\"/health\")\n",
        "    async def health_check():\n",
        "        return service.get_health()\n",
        "    \n",
        "    @app.get(\"/model/info\")\n",
        "    async def model_info():\n",
        "        return service.get_model_info()\n",
        "    \n",
        "    @app.post(\"/predict\")\n",
        "    async def predict(request: PredictionRequest):\n",
        "        result = service.predict_single(request.features)\n",
        "        if 'error' in result:\n",
        "            raise HTTPException(status_code=400, detail=result['error'])\n",
        "        return result\n",
        "    \n",
        "    @app.post(\"/predict/batch\")\n",
        "    async def predict_batch(request: BatchPredictionRequest):\n",
        "        result = service.predict_batch(request.features_list)\n",
        "        if 'error' in result:\n",
        "            raise HTTPException(status_code=400, detail=result['error'])\n",
        "        return result\n",
        "    \n",
        "    return app\n",
        "\n",
        "# API app'i oluÅŸtur\n",
        "api_app = create_fastapi_app()\n",
        "\n",
        "print(\"ğŸŒ FastAPI application oluÅŸturuldu!\")\n",
        "print(\"ğŸ“– API Documentation: http://localhost:8000/docs\")\n",
        "print(\"ğŸ¥ Health Check: http://localhost:8000/health\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. API Testing ve Validation\n",
        "\n",
        "API'mizi test edelim ve Ã¶rnek tahminler yapalÄ±m:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# API testi iÃ§in Ã¶rnek veriler hazÄ±rlayalÄ±m\n",
        "class APITester:\n",
        "    \"\"\"API test sÄ±nÄ±fÄ±\"\"\"\n",
        "    \n",
        "    def __init__(self, service):\n",
        "        self.service = service\n",
        "        self.test_results = {}\n",
        "    \n",
        "    def test_single_prediction(self):\n",
        "        \"\"\"Tek tahmin testi\"\"\"\n",
        "        print(\"ğŸ” Tek tahmin testi...\")\n",
        "        \n",
        "        # Test verisinden Ã¶rnek al\n",
        "        sample_features = X_test.iloc[0].tolist()\n",
        "        actual_value = y_test.iloc[0]\n",
        "        \n",
        "        # Tahmin yap\n",
        "        result = self.service.predict_single(sample_features)\n",
        "        \n",
        "        if 'error' not in result:\n",
        "            predicted_value = result['prediction']\n",
        "            error = abs(predicted_value - actual_value)\n",
        "            \n",
        "            print(f\"âœ… Tek tahmin baÅŸarÄ±lÄ±!\")\n",
        "            print(f\"   ğŸ“Š GerÃ§ek deÄŸer: ${actual_value:.2f}00k\")\n",
        "            print(f\"   ğŸ“Š Tahmin deÄŸer: ${predicted_value:.2f}00k\")\n",
        "            print(f\"   ğŸ“Š Hata: ${error:.2f}00k\")\n",
        "            print(f\"   ğŸ†” Prediction ID: {result['prediction_id']}\")\n",
        "            \n",
        "            self.test_results['single_prediction'] = {\n",
        "                'success': True,\n",
        "                'actual': actual_value,\n",
        "                'predicted': predicted_value,\n",
        "                'error': error\n",
        "            }\n",
        "        else:\n",
        "            print(f\"âŒ Tek tahmin hatasÄ±: {result['error']}\")\n",
        "            self.test_results['single_prediction'] = {'success': False, 'error': result['error']}\n",
        "    \n",
        "    def test_batch_prediction(self, batch_size=5):\n",
        "        \"\"\"Batch tahmin testi\"\"\"\n",
        "        print(f\"\\nğŸ” Batch tahmin testi ({batch_size} Ã¶rnek)...\")\n",
        "        \n",
        "        # Test verisinden batch al\n",
        "        batch_features = X_test.iloc[:batch_size].values.tolist()\n",
        "        actual_values = y_test.iloc[:batch_size].tolist()\n",
        "        \n",
        "        # Batch tahmin yap\n",
        "        result = self.service.predict_batch(batch_features)\n",
        "        \n",
        "        if 'error' not in result:\n",
        "            predictions = [pred['prediction'] for pred in result['predictions']]\n",
        "            errors = [abs(pred - actual) for pred, actual in zip(predictions, actual_values)]\n",
        "            mean_error = np.mean(errors)\n",
        "            \n",
        "            print(f\"âœ… Batch tahmin baÅŸarÄ±lÄ±!\")\n",
        "            print(f\"   ğŸ“Š Batch size: {result['batch_size']}\")\n",
        "            print(f\"   ğŸ“Š Ortalama hata: ${mean_error:.2f}00k\")\n",
        "            print(f\"   ğŸ“Š Min hata: ${min(errors):.2f}00k\")\n",
        "            print(f\"   ğŸ“Š Max hata: ${max(errors):.2f}00k\")\n",
        "            \n",
        "            self.test_results['batch_prediction'] = {\n",
        "                'success': True,\n",
        "                'batch_size': batch_size,\n",
        "                'mean_error': mean_error,\n",
        "                'predictions': predictions,\n",
        "                'actual_values': actual_values\n",
        "            }\n",
        "        else:\n",
        "            print(f\"âŒ Batch tahmin hatasÄ±: {result['error']}\")\n",
        "            self.test_results['batch_prediction'] = {'success': False, 'error': result['error']}\n",
        "    \n",
        "    def test_health_check(self):\n",
        "        \"\"\"Health check testi\"\"\"\n",
        "        print(f\"\\nğŸ¥ Health check testi...\")\n",
        "        \n",
        "        health = self.service.get_health()\n",
        "        \n",
        "        print(f\"âœ… Health check sonuÃ§larÄ±:\")\n",
        "        print(f\"   ğŸ“Š Status: {health['status']}\")\n",
        "        print(f\"   ğŸ“Š Uptime: {health['uptime_seconds']:.2f} saniye\")\n",
        "        print(f\"   ğŸ“Š Total predictions: {health['total_predictions']}\")\n",
        "        print(f\"   ğŸ“Š Model version: {health['model_version']}\")\n",
        "        \n",
        "        self.test_results['health_check'] = health\n",
        "    \n",
        "    def test_model_info(self):\n",
        "        \\\"\\\"\\\"Model info testi\\\"\\\"\\\"\n",
        "        print(f\\\"\\\\nğŸ“‹ Model info testi...\\\")\n",
        "        \n",
        "        info = self.service.get_model_info()\n",
        "        \n",
        "        print(f\\\"âœ… Model bilgileri:\\\")\n",
        "        print(f\\\"   ğŸ“Š Model type: {info['metadata']['model_type']}\\\")\n",
        "        print(f\\\"   ğŸ“Š Version: {info['metadata']['version']}\\\")\n",
        "        print(f\\\"   ğŸ“Š Training date: {info['metadata']['training_date'][:10]}\\\")\n",
        "        print(f\\\"   ğŸ“Š Test RÂ²: {info['metadata']['test_r2']:.4f}\\\")\n",
        "        print(f\\\"   ğŸ“Š Input features: {info['metadata']['input_shape']}\\\")\n",
        "        \n",
        "        self.test_results['model_info'] = info\n",
        "    \n",
        "    def test_error_handling(self):\n",
        "        \\\"\\\"\\\"Error handling testi\\\"\\\"\\\"\n",
        "        print(f\\\"\\\\nâš ï¸ Error handling testi...\\\")\n",
        "        \n",
        "        # YanlÄ±ÅŸ feature sayÄ±sÄ± ile test\n",
        "        wrong_features = [1, 2, 3]  # 8 olmasÄ± gerekiyor\n",
        "        result = self.service.predict_single(wrong_features)\n",
        "        \n",
        "        if 'error' in result:\n",
        "            print(f\\\"âœ… Error handling Ã§alÄ±ÅŸÄ±yor: {result['error']}\\\")\n",
        "            self.test_results['error_handling'] = {'success': True, 'error_message': result['error']}\n",
        "        else:\n",
        "            print(f\\\"âŒ Error handling Ã§alÄ±ÅŸmÄ±yor!\\\")\n",
        "            self.test_results['error_handling'] = {'success': False}\n",
        "    \n",
        "    def run_all_tests(self):\n",
        "        \\\"\\\"\\\"TÃ¼m testleri Ã§alÄ±ÅŸtÄ±r\\\"\\\"\\\"\n",
        "        print(\\\"ğŸ§ª API TESTLER BAÅLATIYOR...\\\")\n",
        "        print(\\\"=\\\" * 50)\n",
        "        \n",
        "        self.test_single_prediction()\n",
        "        self.test_batch_prediction()\n",
        "        self.test_health_check()\n",
        "        self.test_model_info()\n",
        "        self.test_error_handling()\n",
        "        \n",
        "        print(\\\"\\\\n\\\" + \\\"=\\\" * 50)\n",
        "        print(\\\"âœ… TÃ¼m testler tamamlandÄ±!\\\")\n",
        "        \n",
        "        return self.test_results\n",
        "\n",
        "# API testlerini Ã§alÄ±ÅŸtÄ±r\n",
        "tester = APITester(model_service)\n",
        "test_results = tester.run_all_tests()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Docker Containerization\n",
        "\n",
        "Modelimizi Docker container'Ä±nda paketleyeceÄŸiz:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DockerDeployment:\n",
        "    \"\"\"Docker deployment sÄ±nÄ±fÄ±\"\"\"\n",
        "    \n",
        "    def __init__(self, app_name=\"housing-predictor\"):\n",
        "        self.app_name = app_name\n",
        "        self.dockerfile_content = None\n",
        "        self.requirements_content = None\n",
        "        self.app_content = None\n",
        "    \n",
        "    def generate_requirements(self):\n",
        "        \"\"\"Requirements.txt dosyasÄ± oluÅŸtur\"\"\"\n",
        "        self.requirements_content = \"\"\"fastapi==0.104.1\n",
        "uvicorn==0.24.0\n",
        "pandas==2.1.4\n",
        "numpy==1.24.3\n",
        "scikit-learn==1.3.2\n",
        "joblib==1.3.2\n",
        "pydantic==2.5.0\n",
        "\"\"\"\n",
        "        \n",
        "        # Requirements dosyasÄ±nÄ± yaz\n",
        "        with open(\"requirements.txt\", \"w\") as f:\n",
        "            f.write(self.requirements_content)\n",
        "        \n",
        "        print(\"ğŸ“¦ requirements.txt oluÅŸturuldu!\")\n",
        "        return self.requirements_content\n",
        "    \n",
        "    def generate_app_file(self):\n",
        "        \"\"\"Standalone app.py dosyasÄ± oluÅŸtur\"\"\"\n",
        "        self.app_content = '''import os\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "\n",
        "# Pydantic modelleri\n",
        "class PredictionRequest(BaseModel):\n",
        "    features: list\n",
        "\n",
        "class BatchPredictionRequest(BaseModel):\n",
        "    features_list: list\n",
        "\n",
        "class ModelService:\n",
        "    \"\"\"Model servisi\"\"\"\n",
        "    \n",
        "    def __init__(self, model_dir=\"models\", version=\"v1\"):\n",
        "        self.model_dir = Path(model_dir)\n",
        "        self.version = version\n",
        "        self.prediction_count = 0\n",
        "        self.start_time = datetime.now().timestamp()\n",
        "        self.load_model()\n",
        "    \n",
        "    def load_model(self):\n",
        "        \"\"\"Modeli yÃ¼kle\"\"\"\n",
        "        version_dir = self.model_dir / self.version\n",
        "        \n",
        "        # Model yÃ¼kle\n",
        "        model_path = version_dir / \"model.pkl\"\n",
        "        self.model = joblib.load(model_path)\n",
        "        \n",
        "        # Scaler yÃ¼kle\n",
        "        scaler_path = version_dir / \"scaler.pkl\"\n",
        "        self.scaler = joblib.load(scaler_path)\n",
        "        \n",
        "        # Metadata yÃ¼kle\n",
        "        metadata_path = version_dir / \"metadata.json\"\n",
        "        with open(metadata_path, 'r') as f:\n",
        "            self.metadata = json.load(f)\n",
        "    \n",
        "    def predict(self, features):\n",
        "        \"\"\"Tahmin yap\"\"\"\n",
        "        X = np.array(features).reshape(1, -1)\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "        prediction = self.model.predict(X_scaled)[0]\n",
        "        \n",
        "        self.prediction_count += 1\n",
        "        \n",
        "        return {\n",
        "            'prediction': float(prediction),\n",
        "            'model_version': self.metadata['version'],\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'prediction_id': self.prediction_count\n",
        "        }\n",
        "\n",
        "# FastAPI app\n",
        "app = FastAPI(\n",
        "    title=\"ğŸ  Housing Price Predictor\",\n",
        "    description=\"ML model service\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "\n",
        "# Model servisi\n",
        "service = ModelService()\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\n",
        "        \"message\": \"ğŸ  Housing Price Predictor API\",\n",
        "        \"version\": \"1.0.0\",\n",
        "        \"status\": \"healthy\"\n",
        "    }\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"uptime\": datetime.now().timestamp() - service.start_time,\n",
        "        \"predictions\": service.prediction_count\n",
        "    }\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "async def predict(request: PredictionRequest):\n",
        "    try:\n",
        "        if len(request.features) != service.metadata['input_shape']:\n",
        "            raise HTTPException(400, f\"Expected {service.metadata['input_shape']} features\")\n",
        "        \n",
        "        return service.predict(request.features)\n",
        "    except Exception as e:\n",
        "        raise HTTPException(400, str(e))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "'''\n",
        "        \n",
        "        # App dosyasÄ±nÄ± yaz\n",
        "        with open(\"app.py\", \"w\") as f:\n",
        "            f.write(self.app_content)\n",
        "        \n",
        "        print(\"ğŸ app.py oluÅŸturuldu!\")\n",
        "        return self.app_content\n",
        "    \n",
        "    def generate_dockerfile(self):\n",
        "        \"\"\"Dockerfile oluÅŸtur\"\"\"\n",
        "        self.dockerfile_content = f\"\"\"# Python 3.9 slim image kullan\n",
        "FROM python:3.9-slim\n",
        "\n",
        "# Working directory ayarla\n",
        "WORKDIR /app\n",
        "\n",
        "# System dependencies\n",
        "RUN apt-get update && apt-get install -y \\\\\n",
        "    gcc \\\\\n",
        "    && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "# Python dependencies\n",
        "COPY requirements.txt .\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Uygulama dosyalarÄ±nÄ± kopyala\n",
        "COPY app.py .\n",
        "COPY models/ ./models/\n",
        "\n",
        "# Port expose et\n",
        "EXPOSE 8000\n",
        "\n",
        "# Health check ekle\n",
        "HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\\\\n",
        "    CMD curl -f http://localhost:8000/health || exit 1\n",
        "\n",
        "# UygulamayÄ± baÅŸlat\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
        "\"\"\"\n",
        "        \n",
        "        # Dockerfile'Ä± yaz\n",
        "        with open(\"Dockerfile\", \"w\") as f:\n",
        "            f.write(self.dockerfile_content)\n",
        "        \n",
        "        print(\"ğŸ³ Dockerfile oluÅŸturuldu!\")\n",
        "        return self.dockerfile_content\n",
        "    \n",
        "    def generate_docker_compose(self):\n",
        "        \"\"\"Docker Compose dosyasÄ± oluÅŸtur\"\"\"\n",
        "        compose_content = f\"\"\"version: '3.8'\n",
        "\n",
        "services:\n",
        "  {self.app_name}:\n",
        "    build: .\n",
        "    ports:\n",
        "      - \"8000:8000\"\n",
        "    environment:\n",
        "      - ENV=production\n",
        "    restart: unless-stopped\n",
        "    healthcheck:\n",
        "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
        "      interval: 30s\n",
        "      timeout: 10s\n",
        "      retries: 3\n",
        "      start_period: 40s\n",
        "\"\"\"\n",
        "        \n",
        "        # Docker compose dosyasÄ±nÄ± yaz\n",
        "        with open(\"docker-compose.yml\", \"w\") as f:\n",
        "            f.write(compose_content)\n",
        "        \n",
        "        print(\"ğŸ”§ docker-compose.yml oluÅŸturuldu!\")\n",
        "        return compose_content\n",
        "    \n",
        "    def generate_all_files(self):\n",
        "        \"\"\"TÃ¼m Docker dosyalarÄ±nÄ± oluÅŸtur\"\"\"\n",
        "        print(\"ğŸ—ï¸ Docker deployment dosyalarÄ± oluÅŸturuluyor...\")\n",
        "        \n",
        "        self.generate_requirements()\n",
        "        self.generate_app_file()\n",
        "        self.generate_dockerfile()\n",
        "        self.generate_docker_compose()\n",
        "        \n",
        "        print(\"\\nâœ… Docker deployment hazÄ±r!\")\n",
        "        print(\"ğŸš€ Ã‡alÄ±ÅŸtÄ±rmak iÃ§in:\")\n",
        "        print(\"   docker-compose up --build\")\n",
        "        print(\"ğŸ“– API docs: http://localhost:8000/docs\")\n",
        "        \n",
        "        return {\n",
        "            'requirements': self.requirements_content,\n",
        "            'app': self.app_content,\n",
        "            'dockerfile': self.dockerfile_content\n",
        "        }\n",
        "\n",
        "# Docker deployment oluÅŸtur\n",
        "docker_deploy = DockerDeployment(\"housing-predictor\")\n",
        "docker_files = docker_deploy.generate_all_files()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Load Testing ve Performance Monitoring\n",
        "\n",
        "API'mizin performansÄ±nÄ± test edelim:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LoadTester:\n",
        "    \"\"\"Load testing sÄ±nÄ±fÄ±\"\"\"\n",
        "    \n",
        "    def __init__(self, service):\n",
        "        self.service = service\n",
        "        self.test_results = {}\n",
        "        \n",
        "    def single_request_benchmark(self, num_requests=100):\n",
        "        \"\"\"Tek request benchmark testi\"\"\"\n",
        "        print(f\"âš¡ Tek request benchmark ({num_requests} istek)...\")\n",
        "        \n",
        "        # Test verisi hazÄ±rla\n",
        "        sample_features = X_test.iloc[0].tolist()\n",
        "        \n",
        "        start_time = time.time()\n",
        "        successful_requests = 0\n",
        "        failed_requests = 0\n",
        "        response_times = []\n",
        "        \n",
        "        for i in range(num_requests):\n",
        "            request_start = time.time()\n",
        "            \n",
        "            try:\n",
        "                result = self.service.predict_single(sample_features)\n",
        "                request_end = time.time()\n",
        "                \n",
        "                if 'error' not in result:\n",
        "                    successful_requests += 1\n",
        "                    response_times.append((request_end - request_start) * 1000)  # ms\n",
        "                else:\n",
        "                    failed_requests += 1\n",
        "                    \n",
        "            except Exception as e:\n",
        "                failed_requests += 1\n",
        "        \n",
        "        total_time = time.time() - start_time\n",
        "        avg_response_time = np.mean(response_times) if response_times else 0\n",
        "        requests_per_second = successful_requests / total_time if total_time > 0 else 0\n",
        "        \n",
        "        print(f\"âœ… Tek request benchmark sonuÃ§larÄ±:\")\n",
        "        print(f\"   ğŸ“Š Toplam istek: {num_requests}\")\n",
        "        print(f\"   ğŸ“Š BaÅŸarÄ±lÄ±: {successful_requests}\")\n",
        "        print(f\"   ğŸ“Š BaÅŸarÄ±sÄ±z: {failed_requests}\")\n",
        "        print(f\"   ğŸ“Š Ortalama yanÄ±t sÃ¼resi: {avg_response_time:.2f} ms\")\n",
        "        print(f\"   ğŸ“Š Min yanÄ±t sÃ¼resi: {min(response_times):.2f} ms\")\n",
        "        print(f\"   ğŸ“Š Max yanÄ±t sÃ¼resi: {max(response_times):.2f} ms\")\n",
        "        print(f\"   ğŸ“Š Request/saniye: {requests_per_second:.2f}\")\n",
        "        \n",
        "        self.test_results['single_request'] = {\n",
        "            'total_requests': num_requests,\n",
        "            'successful': successful_requests,\n",
        "            'failed': failed_requests,\n",
        "            'avg_response_time_ms': avg_response_time,\n",
        "            'min_response_time_ms': min(response_times) if response_times else 0,\n",
        "            'max_response_time_ms': max(response_times) if response_times else 0,\n",
        "            'requests_per_second': requests_per_second,\n",
        "            'total_time_seconds': total_time\n",
        "        }\n",
        "        \n",
        "        return self.test_results['single_request']\n",
        "    \n",
        "    def concurrent_request_benchmark(self, num_threads=10, requests_per_thread=10):\n",
        "        \"\"\"Concurrent request benchmark testi\"\"\"\n",
        "        print(f\"ğŸš€ Concurrent benchmark ({num_threads} thread, {requests_per_thread} istek/thread)...\")\n",
        "        \n",
        "        sample_features = X_test.iloc[0].tolist()\n",
        "        all_response_times = []\n",
        "        successful_requests = 0\n",
        "        failed_requests = 0\n",
        "        \n",
        "        def worker():\n",
        "            nonlocal successful_requests, failed_requests\n",
        "            thread_response_times = []\n",
        "            \n",
        "            for _ in range(requests_per_thread):\n",
        "                request_start = time.time()\n",
        "                \n",
        "                try:\n",
        "                    result = self.service.predict_single(sample_features)\n",
        "                    request_end = time.time()\n",
        "                    \n",
        "                    if 'error' not in result:\n",
        "                        successful_requests += 1\n",
        "                        thread_response_times.append((request_end - request_start) * 1000)\n",
        "                    else:\n",
        "                        failed_requests += 1\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    failed_requests += 1\n",
        "            \n",
        "            all_response_times.extend(thread_response_times)\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Thread pool ile test Ã§alÄ±ÅŸtÄ±r\n",
        "        with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "            futures = [executor.submit(worker) for _ in range(num_threads)]\n",
        "            \n",
        "            # TÃ¼m thread'lerin bitmesini bekle\n",
        "            for future in futures:\n",
        "                future.result()\n",
        "        \n",
        "        total_time = time.time() - start_time\n",
        "        total_requests = num_threads * requests_per_thread\n",
        "        avg_response_time = np.mean(all_response_times) if all_response_times else 0\n",
        "        requests_per_second = successful_requests / total_time if total_time > 0 else 0\n",
        "        \n",
        "        print(f\"âœ… Concurrent benchmark sonuÃ§larÄ±:\")\n",
        "        print(f\"   ğŸ“Š Toplam thread: {num_threads}\")\n",
        "        print(f\"   ğŸ“Š Thread baÅŸÄ±na istek: {requests_per_thread}\")\n",
        "        print(f\"   ğŸ“Š Toplam istek: {total_requests}\")\n",
        "        print(f\"   ğŸ“Š BaÅŸarÄ±lÄ±: {successful_requests}\")\n",
        "        print(f\"   ğŸ“Š BaÅŸarÄ±sÄ±z: {failed_requests}\")\n",
        "        print(f\"   ğŸ“Š Ortalama yanÄ±t sÃ¼resi: {avg_response_time:.2f} ms\")\n",
        "        print(f\"   ğŸ“Š Request/saniye: {requests_per_second:.2f}\")\n",
        "        \n",
        "        self.test_results['concurrent'] = {\n",
        "            'num_threads': num_threads,\n",
        "            'requests_per_thread': requests_per_thread,\n",
        "            'total_requests': total_requests,\n",
        "            'successful': successful_requests,\n",
        "            'failed': failed_requests,\n",
        "            'avg_response_time_ms': avg_response_time,\n",
        "            'requests_per_second': requests_per_second,\n",
        "            'total_time_seconds': total_time\n",
        "        }\n",
        "        \n",
        "        return self.test_results['concurrent']\n",
        "    \n",
        "    def memory_usage_test(self, num_predictions=1000):\n",
        "        \"\"\"Memory usage testi\"\"\"\n",
        "        print(f\"ğŸ’¾ Memory usage testi ({num_predictions} tahmin)...\")\n",
        "        \n",
        "        # BaÅŸlangÄ±Ã§ memory kullanÄ±mÄ±\n",
        "        process = psutil.Process()\n",
        "        initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
        "        \n",
        "        sample_features = X_test.iloc[0].tolist()\n",
        "        \n",
        "        # Tahminler yap\n",
        "        for i in range(num_predictions):\n",
        "            result = self.service.predict_single(sample_features)\n",
        "            \n",
        "            # Her 100 tahminde bir memory kontrolÃ¼\n",
        "            if i % 100 == 0:\n",
        "                current_memory = process.memory_info().rss / 1024 / 1024\n",
        "                print(f\"   ğŸ“Š {i} tahmin - Memory: {current_memory:.2f} MB\")\n",
        "        \n",
        "        # Final memory kullanÄ±mÄ±\n",
        "        final_memory = process.memory_info().rss / 1024 / 1024\n",
        "        memory_increase = final_memory - initial_memory\n",
        "        \n",
        "        print(f\"âœ… Memory usage sonuÃ§larÄ±:\")\n",
        "        print(f\"   ğŸ“Š BaÅŸlangÄ±Ã§ memory: {initial_memory:.2f} MB\")\n",
        "        print(f\"   ğŸ“Š Final memory: {final_memory:.2f} MB\")\n",
        "        print(f\"   ğŸ“Š Memory artÄ±ÅŸÄ±: {memory_increase:.2f} MB\")\n",
        "        print(f\"   ğŸ“Š Tahmin baÅŸÄ±na memory: {memory_increase/num_predictions*1000:.4f} KB\")\n",
        "        \n",
        "        self.test_results['memory'] = {\n",
        "            'initial_memory_mb': initial_memory,\n",
        "            'final_memory_mb': final_memory,\n",
        "            'memory_increase_mb': memory_increase,\n",
        "            'memory_per_prediction_kb': memory_increase/num_predictions*1000,\n",
        "            'num_predictions': num_predictions\n",
        "        }\n",
        "        \n",
        "        return self.test_results['memory']\n",
        "    \n",
        "    def plot_performance_results(self):\n",
        "        \"\"\"Performance sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtir\"\"\"\n",
        "        if not self.test_results:\n",
        "            print(\"âŒ HenÃ¼z test yapÄ±lmamÄ±ÅŸ!\")\n",
        "            return\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        \n",
        "        # Response time comparison\n",
        "        if 'single_request' in self.test_results and 'concurrent' in self.test_results:\n",
        "            response_times = [\n",
        "                self.test_results['single_request']['avg_response_time_ms'],\n",
        "                self.test_results['concurrent']['avg_response_time_ms']\n",
        "            ]\n",
        "            test_types = ['Single Thread', 'Concurrent']\n",
        "            \n",
        "            axes[0, 0].bar(test_types, response_times, alpha=0.7, color=['blue', 'orange'])\n",
        "            axes[0, 0].set_title('ğŸ“Š Average Response Time')\n",
        "            axes[0, 0].set_ylabel('Response Time (ms)')\n",
        "            axes[0, 0].grid(True, alpha=0.3)\n",
        "            \n",
        "            # Add value labels\n",
        "            for i, v in enumerate(response_times):\n",
        "                axes[0, 0].text(i, v + max(response_times)*0.01, f'{v:.2f} ms', \n",
        "                               ha='center', va='bottom')\n",
        "        \n",
        "        # Requests per second comparison\n",
        "        if 'single_request' in self.test_results and 'concurrent' in self.test_results:\n",
        "            rps_values = [\n",
        "                self.test_results['single_request']['requests_per_second'],\n",
        "                self.test_results['concurrent']['requests_per_second']\n",
        "            ]\n",
        "            \n",
        "            axes[0, 1].bar(test_types, rps_values, alpha=0.7, color=['green', 'red'])\n",
        "            axes[0, 1].set_title('ğŸš€ Requests Per Second')\n",
        "            axes[0, 1].set_ylabel('Requests/Second')\n",
        "            axes[0, 1].grid(True, alpha=0.3)\n",
        "            \n",
        "            # Add value labels\n",
        "            for i, v in enumerate(rps_values):\n",
        "                axes[0, 1].text(i, v + max(rps_values)*0.01, f'{v:.2f}', \n",
        "                               ha='center', va='bottom')\n",
        "        \n",
        "        # Success rate\n",
        "        if 'single_request' in self.test_results:\n",
        "            single_success_rate = (self.test_results['single_request']['successful'] / \n",
        "                                 self.test_results['single_request']['total_requests'] * 100)\n",
        "            concurrent_success_rate = (self.test_results['concurrent']['successful'] / \n",
        "                                     self.test_results['concurrent']['total_requests'] * 100)\n",
        "            \n",
        "            success_rates = [single_success_rate, concurrent_success_rate]\n",
        "            \n",
        "            axes[1, 0].bar(test_types, success_rates, alpha=0.7, color=['lightgreen', 'lightcoral'])\n",
        "            axes[1, 0].set_title('âœ… Success Rate')\n",
        "            axes[1, 0].set_ylabel('Success Rate (%)')\n",
        "            axes[1, 0].set_ylim(0, 105)\n",
        "            axes[1, 0].grid(True, alpha=0.3)\n",
        "            \n",
        "            # Add value labels\n",
        "            for i, v in enumerate(success_rates):\n",
        "                axes[1, 0].text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom')\n",
        "        \n",
        "        # Memory usage\n",
        "        if 'memory' in self.test_results:\n",
        "            memory_data = self.test_results['memory']\n",
        "            memory_categories = ['Initial', 'Final', 'Increase']\n",
        "            memory_values = [\n",
        "                memory_data['initial_memory_mb'],\n",
        "                memory_data['final_memory_mb'],\n",
        "                memory_data['memory_increase_mb']\n",
        "            ]\n",
        "            \n",
        "            bars = axes[1, 1].bar(memory_categories, memory_values, alpha=0.7, \n",
        "                                color=['lightblue', 'orange', 'red'])\n",
        "            axes[1, 1].set_title('ğŸ’¾ Memory Usage')\n",
        "            axes[1, 1].set_ylabel('Memory (MB)')\n",
        "            axes[1, 1].grid(True, alpha=0.3)\n",
        "            \n",
        "            # Add value labels\n",
        "            for bar, value in zip(bars, memory_values):\n",
        "                axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(memory_values)*0.01,\n",
        "                               f'{value:.2f} MB', ha='center', va='bottom')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def run_all_tests(self):\n",
        "        \"\"\"TÃ¼m performance testlerini Ã§alÄ±ÅŸtÄ±r\"\"\"\n",
        "        print(\"âš¡ LOAD TESTING BAÅLATIYOR...\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        # Single request benchmark\n",
        "        self.single_request_benchmark(num_requests=100)\n",
        "        print()\n",
        "        \n",
        "        # Concurrent benchmark\n",
        "        self.concurrent_request_benchmark(num_threads=5, requests_per_thread=20)\n",
        "        print()\n",
        "        \n",
        "        # Memory usage test\n",
        "        self.memory_usage_test(num_predictions=500)\n",
        "        print()\n",
        "        \n",
        "        # Results visualization\n",
        "        self.plot_performance_results()\n",
        "        \n",
        "        print(\"=\" * 60)\n",
        "        print(\"âœ… TÃ¼m performance testleri tamamlandÄ±!\")\n",
        "        \n",
        "        return self.test_results\n",
        "\n",
        "# Load testing Ã§alÄ±ÅŸtÄ±r\n",
        "load_tester = LoadTester(model_service)\n",
        "performance_results = load_tester.run_all_tests()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Ã–zet ve SonuÃ§lar\n",
        "\n",
        "### ğŸ¯ Bu BÃ¶lÃ¼mde Neler Ã–ÄŸrendik?\n",
        "\n",
        "1. **Model Packaging**: Modelleri production-ready hale getirme\n",
        "2. **REST API Development**: FastAPI ile profesyonel API geliÅŸtirme\n",
        "3. **API Testing**: KapsamlÄ± test stratejileri\n",
        "4. **Docker Containerization**: Modelleri containerize etme\n",
        "5. **Load Testing**: Performance ve scalability analizi\n",
        "6. **Monitoring**: System health ve performance tracking\n",
        "\n",
        "### ğŸ“Š Deployment Best Practices:\n",
        "\n",
        "- **Model Versioning**: Her model versiyonunu ayrÄ± dizinlerde saklama\n",
        "- **Error Handling**: Comprehensive hata yÃ¶netimi\n",
        "- **Health Checks**: System durumu monitoring\n",
        "- **Load Testing**: Production Ã¶ncesi performance testi\n",
        "- **Documentation**: API documentation (FastAPI Swagger)\n",
        "- **Containerization**: Docker ile portable deployment\n",
        "\n",
        "### ğŸš€ Production Deployment Checklist:\n",
        "\n",
        "#### âœ… Model HazÄ±rlÄ±ÄŸÄ±\n",
        "- [ ] Model eÄŸitildi ve test edildi\n",
        "- [ ] Preprocessing pipeline kaydedildi\n",
        "- [ ] Model metadata oluÅŸturuldu\n",
        "- [ ] Model versioning sistemi kuruldu\n",
        "\n",
        "#### âœ… API Development\n",
        "- [ ] FastAPI application oluÅŸturuldu\n",
        "- [ ] Input validation implement edildi\n",
        "- [ ] Error handling eklendi\n",
        "- [ ] Health check endpoint'i eklendi\n",
        "- [ ] API documentation hazÄ±rlandÄ±\n",
        "\n",
        "#### âœ… Testing\n",
        "- [ ] Unit testler yazÄ±ldÄ±\n",
        "- [ ] Integration testler yapÄ±ldÄ±\n",
        "- [ ] Load testing tamamlandÄ±\n",
        "- [ ] Error scenarios test edildi\n",
        "\n",
        "#### âœ… Deployment\n",
        "- [ ] Dockerfile oluÅŸturuldu\n",
        "- [ ] Docker Compose konfigÃ¼rasyonu hazÄ±rlandÄ±\n",
        "- [ ] Environment variables tanÄ±mlandÄ±\n",
        "- [ ] Health checks konfigÃ¼re edildi\n",
        "\n",
        "#### âœ… Monitoring\n",
        "- [ ] Performance metrics toplanÄ±yor\n",
        "- [ ] Error logging aktif\n",
        "- [ ] Health monitoring kuruldu\n",
        "- [ ] Alerting sistemi aktif\n",
        "\n",
        "### ğŸ’¡ Advanced Deployment Strategies:\n",
        "\n",
        "1. **Blue-Green Deployment**: Zero-downtime deployment\n",
        "2. **Canary Deployment**: Gradual rollout\n",
        "3. **A/B Testing**: Model comparison in production\n",
        "4. **Auto-scaling**: Traffic'e gÃ¶re scaling\n",
        "5. **Circuit Breaker**: Error durumunda koruma\n",
        "6. **Rate Limiting**: API abuse korumasÄ±\n",
        "\n",
        "### ğŸ”§ Production Tools:\n",
        "\n",
        "- **Container Orchestration**: Kubernetes, Docker Swarm\n",
        "- **API Gateway**: Kong, Ambassador, AWS API Gateway\n",
        "- **Monitoring**: Prometheus, Grafana, ELK Stack\n",
        "- **CI/CD**: Jenkins, GitLab CI, GitHub Actions\n",
        "- **Cloud Platforms**: AWS, Azure, GCP\n",
        "- **Load Balancing**: Nginx, HAProxy, AWS ALB\n",
        "\n",
        "### ğŸ“ˆ Sonraki AdÄ±mlar:\n",
        "\n",
        "1. **Level 6 - Monitoring**: Model performance monitoring\n",
        "2. **A/B Testing**: FarklÄ± model versiyonlarÄ±nÄ± test etme\n",
        "3. **Automated Retraining**: Data drift detection ve otomatik model gÃ¼ncelleme\n",
        "4. **Multi-model Serving**: Birden fazla modeli aynÄ± API'de servis etme\n",
        "\n",
        "---\n",
        "\n",
        "**Level 5 TamamlandÄ±! ğŸ‰**\n",
        "\n",
        "ArtÄ±k ML modellerinizi production ortamÄ±nda gÃ¼venle deploy edebilir, test edebilir ve monitor edebilirsiniz!\n",
        "\n",
        "### ğŸ“š Ek Kaynaklar:\n",
        "\n",
        "- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n",
        "- [Docker Best Practices](https://docs.docker.com/develop/best-practices/)\n",
        "- [MLOps Deployment Patterns](https://ml-ops.org/content/deployment-patterns)\n",
        "- [Model Serving Strategies](https://developers.google.com/machine-learning/guides/rules-of-ml)\n",
        "\n",
        "**ğŸš€ Production'a hazÄ±r ML servisi oluÅŸturdunuz!**\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 6. Ã–zet ve SonuÃ§lar\n",
        "\n",
        "### ğŸ¯ Bu BÃ¶lÃ¼mde Neler Ã–ÄŸrendik?\n",
        "\n",
        "1. **Model Packaging**: Modelleri production-ready hale getirme\n",
        "2. **REST API Development**: FastAPI ile profesyonel API geliÅŸtirme\n",
        "3. **API Testing**: KapsamlÄ± test stratejileri\n",
        "4. **Docker Containerization**: Modelleri containerize etme\n",
        "5. **Load Testing**: Performance ve scalability analizi\n",
        "6. **Monitoring**: System health ve performance tracking\n",
        "\n",
        "### ğŸ“Š Deployment Best Practices:\n",
        "\n",
        "- **Model Versioning**: Her model versiyonunu ayrÄ± dizinlerde saklama\n",
        "- **Error Handling**: Comprehensive hata yÃ¶netimi\n",
        "- **Health Checks**: System durumu monitoring\n",
        "- **Load Testing**: Production Ã¶ncesi performance testi\n",
        "- **Documentation**: API documentation (FastAPI Swagger)\n",
        "- **Containerization**: Docker ile portable deployment\n",
        "\n",
        "### ğŸš€ Production Deployment Checklist:\n",
        "\n",
        "#### âœ… Model HazÄ±rlÄ±ÄŸÄ±\n",
        "- [ ] Model eÄŸitildi ve test edildi\n",
        "- [ ] Preprocessing pipeline kaydedildi\n",
        "- [ ] Model metadata oluÅŸturuldu\n",
        "- [ ] Model versioning sistemi kuruldu\n",
        "\n",
        "#### âœ… API Development\n",
        "- [ ] FastAPI application oluÅŸturuldu\n",
        "- [ ] Input validation implement edildi\n",
        "- [ ] Error handling eklendi\n",
        "- [ ] Health check endpoint'i eklendi\n",
        "- [ ] API documentation hazÄ±rlandÄ±\n",
        "\n",
        "#### âœ… Testing\n",
        "- [ ] Unit testler yazÄ±ldÄ±\n",
        "- [ ] Integration testler yapÄ±ldÄ±\n",
        "- [ ] Load testing tamamlandÄ±\n",
        "- [ ] Error scenarios test edildi\n",
        "\n",
        "#### âœ… Deployment\n",
        "- [ ] Dockerfile oluÅŸturuldu\n",
        "- [ ] Docker Compose konfigÃ¼rasyonu hazÄ±rlandÄ±\n",
        "- [ ] Environment variables tanÄ±mlandÄ±\n",
        "- [ ] Health checks konfigÃ¼re edildi\n",
        "\n",
        "#### âœ… Monitoring\n",
        "- [ ] Performance metrics toplanÄ±yor\n",
        "- [ ] Error logging aktif\n",
        "- [ ] Health monitoring kuruldu\n",
        "- [ ] Alerting sistemi aktif\n",
        "\n",
        "### ğŸ’¡ Advanced Deployment Strategies:\n",
        "\n",
        "1. **Blue-Green Deployment**: Zero-downtime deployment\n",
        "2. **Canary Deployment**: Gradual rollout\n",
        "3. **A/B Testing**: Model comparison in production\n",
        "4. **Auto-scaling**: Traffic'e gÃ¶re scaling\n",
        "5. **Circuit Breaker**: Error durumunda koruma\n",
        "6. **Rate Limiting**: API abuse korumasÄ±\n",
        "\n",
        "### ğŸ”§ Production Tools:\n",
        "\n",
        "- **Container Orchestration**: Kubernetes, Docker Swarm\n",
        "- **API Gateway**: Kong, Ambassador, AWS API Gateway\n",
        "- **Monitoring**: Prometheus, Grafana, ELK Stack\n",
        "- **CI/CD**: Jenkins, GitLab CI, GitHub Actions\n",
        "- **Cloud Platforms**: AWS, Azure, GCP\n",
        "- **Load Balancing**: Nginx, HAProxy, AWS ALB\n",
        "\n",
        "### ğŸ“ˆ Sonraki AdÄ±mlar:\n",
        "\n",
        "1. **Level 6 - Monitoring**: Model performance monitoring\n",
        "2. **A/B Testing**: FarklÄ± model versiyonlarÄ±nÄ± test etme\n",
        "3. **Automated Retraining**: Data drift detection ve otomatik model gÃ¼ncelleme\n",
        "4. **Multi-model Serving**: Birden fazla modeli aynÄ± API'de servis etme\n",
        "\n",
        "---\n",
        "\n",
        "**Level 5 TamamlandÄ±! ğŸ‰**\n",
        "\n",
        "ArtÄ±k ML modellerinizi production ortamÄ±nda gÃ¼venle deploy edebilir, test edebilir ve monitor edebilirsiniz!\n",
        "\n",
        "### ğŸ“š Ek Kaynaklar:\n",
        "\n",
        "- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n",
        "- [Docker Best Practices](https://docs.docker.com/develop/best-practices/)\n",
        "- [MLOps Deployment Patterns](https://ml-ops.org/content/deployment-patterns)\n",
        "- [Model Serving Strategies](https://developers.google.com/machine-learning/guides/rules-of-ml)\n",
        "\n",
        "**ğŸš€ Production'a hazÄ±r ML servisi oluÅŸturdunuz!**\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 6. Ã–zet ve SonuÃ§lar\n",
        "\n",
        "### ğŸ¯ Bu BÃ¶lÃ¼mde Neler Ã–ÄŸrendik?\n",
        "\n",
        "1. **Model Packaging**: Modelleri production-ready hale getirme\n",
        "2. **REST API Development**: FastAPI ile profesyonel API geliÅŸtirme\n",
        "3. **API Testing**: KapsamlÄ± test stratejileri\n",
        "4. **Docker Containerization**: Modelleri containerize etme\n",
        "5. **Load Testing**: Performance ve scalability analizi\n",
        "6. **Monitoring**: System health ve performance tracking\n",
        "\n",
        "### ğŸ“Š Deployment Best Practices:\n",
        "\n",
        "- **Model Versioning**: Her model versiyonunu ayrÄ± dizinlerde saklama\n",
        "- **Error Handling**: Comprehensive hata yÃ¶netimi\n",
        "- **Health Checks**: System durumu monitoring\n",
        "- **Load Testing**: Production Ã¶ncesi performance testi\n",
        "- **Documentation**: API documentation (FastAPI Swagger)\n",
        "- **Containerization**: Docker ile portable deployment\n",
        "\n",
        "### ğŸš€ Production Deployment Checklist:\n",
        "\n",
        "#### âœ… Model HazÄ±rlÄ±ÄŸÄ±\n",
        "- [ ] Model eÄŸitildi ve test edildi\n",
        "- [ ] Preprocessing pipeline kaydedildi\n",
        "- [ ] Model metadata oluÅŸturuldu\n",
        "- [ ] Model versioning sistemi kuruldu\n",
        "\n",
        "#### âœ… API Development\n",
        "- [ ] FastAPI application oluÅŸturuldu\n",
        "- [ ] Input validation implement edildi\n",
        "- [ ] Error handling eklendi\n",
        "- [ ] Health check endpoint'i eklendi\n",
        "- [ ] API documentation hazÄ±rlandÄ±\n",
        "\n",
        "#### âœ… Testing\n",
        "- [ ] Unit testler yazÄ±ldÄ±\n",
        "- [ ] Integration testler yapÄ±ldÄ±\n",
        "- [ ] Load testing tamamlandÄ±\n",
        "- [ ] Error scenarios test edildi\n",
        "\n",
        "#### âœ… Deployment\n",
        "- [ ] Dockerfile oluÅŸturuldu\n",
        "- [ ] Docker Compose konfigÃ¼rasyonu hazÄ±rlandÄ±\n",
        "- [ ] Environment variables tanÄ±mlandÄ±\n",
        "- [ ] Health checks konfigÃ¼re edildi\n",
        "\n",
        "#### âœ… Monitoring\n",
        "- [ ] Performance metrics toplanÄ±yor\n",
        "- [ ] Error logging aktif\n",
        "- [ ] Health monitoring kuruldu\n",
        "- [ ] Alerting sistemi aktif\n",
        "\n",
        "### ğŸ’¡ Advanced Deployment Strategies:\n",
        "\n",
        "1. **Blue-Green Deployment**: Zero-downtime deployment\n",
        "2. **Canary Deployment**: Gradual rollout\n",
        "3. **A/B Testing**: Model comparison in production\n",
        "4. **Auto-scaling**: Traffic'e gÃ¶re scaling\n",
        "5. **Circuit Breaker**: Error durumunda koruma\n",
        "6. **Rate Limiting**: API abuse korumasÄ±\n",
        "\n",
        "### ğŸ”§ Production Tools:\n",
        "\n",
        "- **Container Orchestration**: Kubernetes, Docker Swarm\n",
        "- **API Gateway**: Kong, Ambassador, AWS API Gateway\n",
        "- **Monitoring**: Prometheus, Grafana, ELK Stack\n",
        "- **CI/CD**: Jenkins, GitLab CI, GitHub Actions\n",
        "- **Cloud Platforms**: AWS, Azure, GCP\n",
        "- **Load Balancing**: Nginx, HAProxy, AWS ALB\n",
        "\n",
        "### ğŸ“ˆ Sonraki AdÄ±mlar:\n",
        "\n",
        "1. **Level 6 - Monitoring**: Model performance monitoring\n",
        "2. **A/B Testing**: FarklÄ± model versiyonlarÄ±nÄ± test etme\n",
        "3. **Automated Retraining**: Data drift detection ve otomatik model gÃ¼ncelleme\n",
        "4. **Multi-model Serving**: Birden fazla modeli aynÄ± API'de servis etme\n",
        "\n",
        "---\n",
        "\n",
        "**Level 5 TamamlandÄ±! ğŸ‰**\n",
        "\n",
        "ArtÄ±k ML modellerinizi production ortamÄ±nda gÃ¼venle deploy edebilir, test edebilir ve monitor edebilirsiniz!\n",
        "\n",
        "### ğŸ“š Ek Kaynaklar:\n",
        "\n",
        "- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n",
        "- [Docker Best Practices](https://docs.docker.com/develop/best-practices/)\n",
        "- [MLOps Deployment Patterns](https://ml-ops.org/content/deployment-patterns)\n",
        "- [Model Serving Strategies](https://developers.google.com/machine-learning/guides/rules-of-ml)\n",
        "\n",
        "**ğŸš€ Production'a hazÄ±r ML servisi oluÅŸturdunuz!**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
