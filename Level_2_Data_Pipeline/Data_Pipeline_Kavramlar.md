# Data Pipeline Kavramlar SÃ¶zlÃ¼ÄŸÃ¼

Bu dosya Level 2: Data Pipeline notebook'unda geÃ§en tÃ¼m kavramlarÄ± detaylÄ± olarak aÃ§Ä±klamaktadÄ±r.

## ğŸ“š Ä°Ã§indekiler
- [Data Pipeline Temelleri](#data-pipeline-temelleri)
- [Veri Versiyonlama](#veri-versiyonlama)
- [Veri Kalitesi ve Validasyon](#veri-kalitesi-ve-validasyon)
- [Veri Temizleme](#veri-temizleme)
- [Feature Engineering](#feature-engineering)
- [Veri Drift ve Monitoring](#veri-drift-ve-monitoring)

---

## Data Pipeline Temelleri

### ğŸ”„ Data Pipeline
**TanÄ±m:** Ham verinin toplanmasÄ±ndan model eÄŸitimi iÃ§in hazÄ±r hale getirilmesine kadar olan tÃ¼m veri iÅŸleme sÃ¼reÃ§lerinin otomatikleÅŸtirilmiÅŸ dizisi.

**Pipeline BileÅŸenleri:**
1. **Data Ingestion (Veri Alma):** Ã‡eÅŸitli kaynaklardan veri toplama
2. **Data Validation (Veri DoÄŸrulama):** Veri kalitesi kontrolÃ¼
3. **Data Transformation (Veri DÃ¶nÃ¼ÅŸtÃ¼rme):** Veriyi uygun formata Ã§evirme
4. **Feature Engineering (Ã–zellik MÃ¼hendisliÄŸi):** Yeni Ã¶zellikler oluÅŸturma
5. **Data Storage (Veri Depolama):** Ä°ÅŸlenmiÅŸ veriyi saklama

### ğŸ¯ ETL vs ELT
**ETL (Extract, Transform, Load):**
- Ã–nce transform, sonra load
- Geleneksel data warehouse yaklaÅŸÄ±mÄ±
- Structured data iÃ§in uygun

**ELT (Extract, Load, Transform):**
- Ã–nce load, sonra transform
- Modern big data yaklaÅŸÄ±mÄ±
- Unstructured data iÃ§in uygun

### ğŸ“Š Data Lineage
**TanÄ±m:** Verinin nereden geldiÄŸi, nasÄ±l iÅŸlendiÄŸi ve nereye gittiÄŸinin izlenmesi.

**FaydalarÄ±:**
- Troubleshooting
- Compliance
- Impact analysis
- Data governance

---

## Veri Versiyonlama

### ğŸ—ƒï¸ DVC (Data Version Control)
**TanÄ±m:** BÃ¼yÃ¼k veri dosyalarÄ± ve ML modelleri iÃ§in Git-benzeri version control sistemi.

**Temel Ã–zellikler:**
- Large file versioning
- Remote storage integration
- Pipeline reproducibility
- Experiment tracking

**Temel Komutlar:**
```bash
# DVC baÅŸlatma
dvc init

# Veri dosyasÄ± ekleme
dvc add data/dataset.csv

# Pipeline tanÄ±mlama
dvc run -d data/raw -o data/processed python process.py

# Remote storage kurulumu
dvc remote add -d storage s3://my-bucket/dvc-storage

# Veriyi remote'a gÃ¶nderme
dvc push

# Veriyi remote'dan Ã§ekme
dvc pull

# Pipeline yeniden Ã§alÄ±ÅŸtÄ±rma
dvc repro
```

### ğŸ“ dvc.yaml
**TanÄ±m:** DVC pipeline'larÄ±nÄ± tanÄ±mlamak iÃ§in kullanÄ±lan YAML dosyasÄ±.

**Ã–rnek YapÄ±:**
```yaml
stages:
  prepare:
    cmd: python prepare.py
    deps:
      - data/raw
    outs:
      - data/prepared
      
  train:
    cmd: python train.py
    deps:
      - data/prepared
      - train.py
    outs:
      - models/model.pkl
    metrics:
      - metrics.json
```

### ğŸ”„ Git vs DVC
**Git:** Kod iÃ§in version control
**DVC:** Veri ve modeller iÃ§in version control
**Birlikte kullanÄ±m:** Tam reproducibility iÃ§in her ikisi gerekli

---

## Veri Kalitesi ve Validasyon

### ğŸ” Data Validation
**TanÄ±m:** Verinin beklenen kalite standartlarÄ±nÄ± karÅŸÄ±layÄ±p karÅŸÄ±lamadÄ±ÄŸÄ±nÄ±n kontrol edilmesi.

**Validation TÃ¼rleri:**
1. **Schema Validation:** Veri tiplerinin ve yapÄ±nÄ±n kontrolÃ¼
2. **Range Validation:** DeÄŸerlerin beklenen aralÄ±klarda olup olmadÄ±ÄŸÄ±
3. **Pattern Validation:** Regex pattern'lerine uygunluk
4. **Business Rule Validation:** Ä°ÅŸ kurallarÄ±na uygunluk

### ğŸ“Š Data Quality Metrics
**Temel Kalite BoyutlarÄ±:**
- **Completeness (TamlÄ±k):** Eksik deÄŸer oranÄ±
- **Accuracy (DoÄŸruluk):** Verinin gerÃ§eÄŸi yansÄ±tma derecesi
- **Consistency (TutarlÄ±lÄ±k):** FarklÄ± kaynaklardaki verinin uyumu
- **Validity (GeÃ§erlilik):** TanÄ±mlÄ± kurallara uygunluk
- **Uniqueness (Benzersizlik):** Duplikasyon oranÄ±
- **Timeliness (GÃ¼ncellik):** Verinin ne kadar gÃ¼ncel olduÄŸu

### ğŸš¨ Data Quality Issues
**YaygÄ±n Problemler:**
```python
# Eksik deÄŸerler
df.isnull().sum()

# Duplikatlar
df.duplicated().sum()

# AykÄ±rÄ± deÄŸerler (IQR yÃ¶ntemi)
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
outliers = df[(df < Q1 - 1.5*IQR) | (df > Q3 + 1.5*IQR)]

# Veri tipi uyumsuzluklarÄ±
df.dtypes
```

### ğŸ›¡ï¸ Great Expectations
**TanÄ±m:** Data validation ve documentation iÃ§in Python kÃ¼tÃ¼phanesi.

**Temel KullanÄ±m:**
```python
import great_expectations as ge

# Dataset oluÅŸturma
df_ge = ge.from_pandas(df)

# Expectations tanÄ±mlama
df_ge.expect_column_values_to_not_be_null('column_name')
df_ge.expect_column_values_to_be_between('numeric_column', 0, 100)
df_ge.expect_column_values_to_match_regex('email', r'^[a-zA-Z0-9+_.-]+@[a-zA-Z0-9.-]+$')
```

---

## Veri Temizleme

### ğŸ§¹ Data Cleaning
**TanÄ±m:** Verinin kalitesini artÄ±rmak iÃ§in hatalÄ±, eksik veya tutarsÄ±z verilerin dÃ¼zeltilmesi sÃ¼reci.

**Temel AdÄ±mlar:**
1. **Missing Value Handling:** Eksik deÄŸerlerin iÅŸlenmesi
2. **Outlier Detection & Treatment:** AykÄ±rÄ± deÄŸerlerin tespit edilmesi ve iÅŸlenmesi
3. **Duplicate Removal:** DuplikatlarÄ±n kaldÄ±rÄ±lmasÄ±
4. **Data Type Conversion:** Veri tiplerinin dÃ¼zeltilmesi
5. **Standardization:** Verilerin standardizasyonu

### ğŸ”§ Missing Value Strategies
**Silme Stratejileri:**
- **Listwise Deletion:** Eksik deÄŸeri olan tÃ¼m satÄ±rlarÄ± silme
- **Pairwise Deletion:** Sadece ilgili analiz iÃ§in eksik deÄŸerleri silme

**Doldurma Stratejileri:**
```python
# Mean/Median/Mode ile doldurma
df['column'].fillna(df['column'].mean())
df['column'].fillna(df['column'].median())
df['column'].fillna(df['column'].mode()[0])

# Forward/Backward fill
df['column'].fillna(method='ffill')
df['column'].fillna(method='bfill')

# Interpolation
df['column'].interpolate()

# Machine learning based imputation
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=5)
df_imputed = imputer.fit_transform(df)
```

### ğŸ“ˆ Outlier Detection Methods
**Ä°statistiksel YÃ¶ntemler:**
```python
# Z-Score method
z_scores = np.abs(stats.zscore(df['column']))
outliers = df[z_scores > 3]

# IQR method
Q1 = df['column'].quantile(0.25)
Q3 = df['column'].quantile(0.75)
IQR = Q3 - Q1
outliers = df[(df['column'] < Q1 - 1.5*IQR) | 
              (df['column'] > Q3 + 1.5*IQR)]
```

**Machine Learning YÃ¶ntemleri:**
```python
# Isolation Forest
from sklearn.ensemble import IsolationForest
iso_forest = IsolationForest(contamination=0.1)
outliers = iso_forest.fit_predict(df)

# Local Outlier Factor
from sklearn.neighbors import LocalOutlierFactor
lof = LocalOutlierFactor(n_neighbors=20)
outliers = lof.fit_predict(df)
```

### ğŸ¯ Outlier Treatment
**Treatment Stratejileri:**
- **Removal:** AykÄ±rÄ± deÄŸerleri silme
- **Capping/Winsorizing:** Belirli deÄŸerlerde sÄ±nÄ±rlama
- **Transformation:** Log, sqrt gibi dÃ¶nÃ¼ÅŸÃ¼mler
- **Binning:** Kategorik gruplara ayÄ±rma

---

## Feature Engineering

### ğŸ”§ Feature Engineering
**TanÄ±m:** Ham veriden makine Ã¶ÄŸrenmesi algoritmalarÄ±nÄ±n daha iyi performans gÃ¶sterebileceÄŸi Ã¶zellikler oluÅŸturma sÃ¼reci.

**Ana Kategoriler:**
1. **Feature Creation:** Yeni Ã¶zellikler oluÅŸturma
2. **Feature Transformation:** Mevcut Ã¶zellikleri dÃ¶nÃ¼ÅŸtÃ¼rme
3. **Feature Selection:** En Ã¶nemli Ã¶zellikleri seÃ§me
4. **Feature Scaling:** Ã–zellikleri Ã¶lÃ§eklendirme

### ğŸ¨ Feature Creation Techniques
**Numerical Features:**
```python
# Mathematical operations
df['feature_ratio'] = df['feature1'] / df['feature2']
df['feature_diff'] = df['feature1'] - df['feature2']
df['feature_product'] = df['feature1'] * df['feature2']

# Polynomial features
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2)
poly_features = poly.fit_transform(df[['feature1', 'feature2']])

# Binning
df['age_group'] = pd.cut(df['age'], bins=[0, 25, 45, 65, 100], 
                        labels=['Young', 'Adult', 'Middle', 'Senior'])
```

**Categorical Features:**
```python
# One-hot encoding
pd.get_dummies(df['category'])

# Label encoding
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['category_encoded'] = le.fit_transform(df['category'])

# Target encoding
df.groupby('category')['target'].mean()
```

**DateTime Features:**
```python
# Extract date components
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day_of_week'] = df['date'].dt.dayofweek
df['is_weekend'] = df['date'].dt.dayofweek.isin([5, 6])

# Time-based features
df['days_since'] = (df['current_date'] - df['reference_date']).dt.days
```

### ğŸ“ Feature Scaling
**Normalization (Min-Max Scaling):**
```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df_normalized = scaler.fit_transform(df)
```

**Standardization (Z-Score):**
```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df_standardized = scaler.fit_transform(df)
```

**Robust Scaling:**
```python
from sklearn.preprocessing import RobustScaler
scaler = RobustScaler()
df_robust = scaler.fit_transform(df)
```

### ğŸ¯ Feature Selection
**Filter Methods:**
```python
# Correlation-based
correlation_matrix = df.corr()
high_corr_features = correlation_matrix[correlation_matrix > 0.9]

# Statistical tests
from sklearn.feature_selection import SelectKBest, f_classif
selector = SelectKBest(f_classif, k=10)
selected_features = selector.fit_transform(X, y)
```

**Wrapper Methods:**
```python
# Recursive Feature Elimination
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

estimator = RandomForestClassifier()
selector = RFE(estimator, n_features_to_select=10)
selected_features = selector.fit_transform(X, y)
```

**Embedded Methods:**
```python
# LASSO regularization
from sklearn.linear_model import Lasso
lasso = Lasso(alpha=0.1)
lasso.fit(X, y)
selected_features = X.columns[lasso.coef_ != 0]
```

---

## Veri Drift ve Monitoring

### ğŸ“Š Data Drift
**TanÄ±m:** Zaman iÃ§inde model input verilerinin istatistiksel Ã¶zelliklerinin deÄŸiÅŸmesi.

**Drift TÃ¼rleri:**
1. **Covariate Shift:** P(X) deÄŸiÅŸir, P(Y|X) sabit kalÄ±r
2. **Prior Probability Shift:** P(Y) deÄŸiÅŸir, P(X|Y) sabit kalÄ±r
3. **Concept Drift:** P(Y|X) deÄŸiÅŸir

### ğŸ” Drift Detection Methods
**Ä°statistiksel Testler:**
```python
# Kolmogorov-Smirnov Test
from scipy import stats
ks_statistic, p_value = stats.ks_2samp(baseline_data, current_data)

# Chi-square Test (categorical data)
chi2, p_value = stats.chisquare(observed, expected)

# Population Stability Index (PSI)
def calculate_psi(expected, actual, buckets=10):
    expected_percents = pd.qcut(expected, buckets, duplicates='drop')
    actual_percents = pd.qcut(actual, buckets, duplicates='drop')
    
    expected_counts = expected_percents.value_counts()
    actual_counts = actual_percents.value_counts()
    
    psi_value = sum((actual_counts/len(actual) - expected_counts/len(expected)) * 
                   np.log((actual_counts/len(actual)) / (expected_counts/len(expected))))
    return psi_value
```

**Distance-based Methods:**
```python
# Wasserstein Distance
from scipy.stats import wasserstein_distance
distance = wasserstein_distance(baseline_data, current_data)

# KL Divergence
from scipy.stats import entropy
kl_div = entropy(baseline_data, current_data)
```

### ğŸš¨ Evidently AI
**TanÄ±m:** ML model ve data monitoring iÃ§in aÃ§Ä±k kaynak kÃ¼tÃ¼phane.

**Temel KullanÄ±m:**
```python
from evidently.report import Report
from evidently.metric_preset import DataDriftPreset, DataQualityPreset

# Data drift report
data_drift_report = Report(metrics=[DataDriftPreset()])
data_drift_report.run(reference_data=baseline_df, current_data=current_df)
data_drift_report.show()

# Data quality report
data_quality_report = Report(metrics=[DataQualityPreset()])
data_quality_report.run(reference_data=baseline_df, current_data=current_df)
```

### ğŸ“ˆ WhyLabs
**TanÄ±m:** ML observability ve monitoring platformu.

**Ã–zellikler:**
- Automated data profiling
- Drift detection
- Data quality monitoring
- Anomaly detection

---

## ğŸ› ï¸ Pipeline Orchestration

### ğŸ“Š Apache Airflow
**TanÄ±m:** Workflow scheduling ve monitoring platformu.

**DAG (Directed Acyclic Graph) Ã–rneÄŸi:**
```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'data_pipeline',
    default_args=default_args,
    description='Data processing pipeline',
    schedule_interval='@daily'
)

def extract_data():
    # Data extraction logic
    pass

def transform_data():
    # Data transformation logic
    pass

def load_data():
    # Data loading logic
    pass

extract_task = PythonOperator(
    task_id='extract',
    python_callable=extract_data,
    dag=dag
)

transform_task = PythonOperator(
    task_id='transform',
    python_callable=transform_data,
    dag=dag
)

load_task = PythonOperator(
    task_id='load',
    python_callable=load_data,
    dag=dag
)

extract_task >> transform_task >> load_task
```

### ğŸ”„ Prefect
**TanÄ±m:** Modern workflow orchestration tool.

**Temel KullanÄ±m:**
```python
import prefect
from prefect import Flow, task

@task
def extract_data():
    return "extracted_data"

@task
def transform_data(data):
    return f"transformed_{data}"

@task
def load_data(data):
    print(f"Loading {data}")

with Flow("data-pipeline") as flow:
    raw_data = extract_data()
    processed_data = transform_data(raw_data)
    load_data(processed_data)

flow.run()
```

---

## ğŸ—„ï¸ Data Storage Solutions

### ğŸ“Š Data Lake vs Data Warehouse
**Data Lake:**
- Schema-on-read
- Raw data storage
- Flexible structure
- Lower cost
- Big data friendly

**Data Warehouse:**
- Schema-on-write
- Processed data storage
- Structured data
- Higher cost
- Analytics optimized

### ğŸ—ï¸ Modern Data Stack
**Typical Components:**
1. **Data Sources:** Applications, APIs, databases
2. **Data Ingestion:** Fivetran, Stitch, Airbyte
3. **Data Storage:** Snowflake, BigQuery, Redshift
4. **Data Transformation:** dbt, Dataform
5. **Business Intelligence:** Tableau, Looker, PowerBI
6. **Data Orchestration:** Airflow, Prefect
7. **Data Quality:** Great Expectations, Monte Carlo

---

## ğŸ”§ Praktik Uygulamalar

### ğŸ“ Pipeline Implementation
```python
class DataPipeline:
    def __init__(self):
        self.steps = []
    
    def add_step(self, step_func, step_name):
        self.steps.append((step_name, step_func))
    
    def run(self, data):
        for step_name, step_func in self.steps:
            print(f"Running {step_name}...")
            data = step_func(data)
            print(f"{step_name} completed.")
        return data

# Pipeline oluÅŸturma
pipeline = DataPipeline()
pipeline.add_step(validate_data, "Data Validation")
pipeline.add_step(clean_data, "Data Cleaning")
pipeline.add_step(engineer_features, "Feature Engineering")

# Pipeline Ã§alÄ±ÅŸtÄ±rma
processed_data = pipeline.run(raw_data)
```

### ğŸ” Data Quality Monitoring
```python
def monitor_data_quality(df):
    quality_report = {
        'total_rows': len(df),
        'total_columns': len(df.columns),
        'missing_values': df.isnull().sum().sum(),
        'duplicate_rows': df.duplicated().sum(),
        'data_types': df.dtypes.to_dict()
    }
    
    # Quality alerts
    if quality_report['missing_values'] > len(df) * 0.1:
        print("âš ï¸  High missing value rate detected!")
    
    if quality_report['duplicate_rows'] > 0:
        print("âš ï¸  Duplicate rows detected!")
    
    return quality_report
```

---

## ğŸ“ Best Practices

### âœ… Pipeline Design
1. **Modularity:** Her step'i ayrÄ± function olarak tasarla
2. **Idempotency:** Pipeline'Ä± birden Ã§ok kez Ã§alÄ±ÅŸtÄ±rabilir ol
3. **Error Handling:** Robust error handling ve logging
4. **Testing:** Unit test'ler ve integration test'ler
5. **Documentation:** Her step'i dokÃ¼mante et

### ğŸ”„ Data Management
1. **Version Control:** Veri ve kod versiyonlarÄ±nÄ± takip et
2. **Backup Strategy:** DÃ¼zenli backup'lar al
3. **Access Control:** Data governance politikalarÄ± uygula
4. **Monitoring:** SÃ¼rekli monitoring ve alerting
5. **Scalability:** BÃ¼yÃ¼yen veri hacmi iÃ§in Ã¶lÃ§eklenebilir tasarÄ±m

### ğŸ“Š Quality Assurance
1. **Data Profiling:** DÃ¼zenli veri profilleme
2. **Validation Rules:** Comprehensive validation rules
3. **Drift Detection:** SÃ¼rekli drift monitoring
4. **Anomaly Detection:** Otomatik anomaly detection
5. **Quality Metrics:** KPI'lar ve dashboard'lar

---

## ğŸ”— Ä°lgili Kaynaklar

- [DVC Documentation](https://dvc.org/doc)
- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Great Expectations Documentation](https://docs.greatexpectations.io/)
- [Evidently AI Documentation](https://docs.evidentlyai.com/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Scikit-learn Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)

---

*Bu dÃ¶kÃ¼man Level 2: Data Pipeline notebook'unda geÃ§en kavramlarÄ± kapsamaktadÄ±r. Her kavram praktik Ã¶rneklerle desteklenmiÅŸtir.* 