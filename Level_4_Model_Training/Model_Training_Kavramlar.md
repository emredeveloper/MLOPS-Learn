# Level 4: Model Training - Kavramlar

## ğŸ¯ Advanced Model Training KavramlarÄ±

### 1. Cross-Validation Stratejileri

#### K-Fold Cross-Validation
- **TanÄ±m**: Veri setini k eÅŸit parÃ§aya bÃ¶ler, k-1 parÃ§a ile eÄŸitir, 1 parÃ§a ile test eder
- **Avantajlar**: Objektif model deÄŸerlendirmesi, overfitting tespiti
- **KullanÄ±m**: Genel regresyon ve sÄ±nÄ±flandÄ±rma problemleri

#### Stratified K-Fold
- **TanÄ±m**: SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± koruyarak veri setini bÃ¶ler
- **Avantajlar**: Dengesiz veri setlerinde gÃ¼venilir sonuÃ§lar
- **KullanÄ±m**: SÄ±nÄ±flandÄ±rma problemleri

#### Time Series Split
- **TanÄ±m**: Zaman serisinde geÃ§miÅŸten geleceÄŸe doÄŸru split yapar
- **Avantajlar**: Temporal dependency'leri korur
- **KullanÄ±m**: Zaman serisi tahmin problemleri

### 2. Hyperparameter Optimization

#### Grid Search
- **TanÄ±m**: TÃ¼m parametre kombinasyonlarÄ±nÄ± sistematik olarak dener
- **Avantajlar**: Garantili en iyi kombinasyon bulma
- **Dezavantajlar**: Exponentially artan hesaplama maliyeti
- **KullanÄ±m**: Az parametreli modellerde

#### Random Search
- **TanÄ±m**: Rastgele parametre kombinasyonlarÄ± dener
- **Avantajlar**: Grid search'ten daha hÄ±zlÄ±, iyi sonuÃ§lar
- **Dezavantajlar**: Optimal sonuÃ§ garantisi yok
- **KullanÄ±m**: Ã‡ok parametreli modellerde

#### Bayesian Optimization
- **TanÄ±m**: GeÃ§miÅŸ deneyimleri kullanarak akÄ±llÄ± parametre seÃ§imi
- **Avantajlar**: En az deneme ile en iyi sonuÃ§
- **Tools**: Optuna, Hyperopt, Gaussian Processes
- **KullanÄ±m**: PahalÄ± hesaplama gerektiren modellerde

### 3. Ensemble Methods

#### Voting
- **Hard Voting**: Ã‡oÄŸunluk oyuyla karar (sÄ±nÄ±flandÄ±rma)
- **Soft Voting**: OlasÄ±lÄ±k ortalamasÄ± (sÄ±nÄ±flandÄ±rma)
- **Averaging**: Tahmin ortalamasÄ± (regresyon)
- **Weighted Voting**: AÄŸÄ±rlÄ±klÄ± oylama

#### Stacking
- **TanÄ±m**: Base modellerin Ã§Ä±ktÄ±larÄ±nÄ± meta-model ile Ã¶ÄŸrenir
- **Avantajlar**: FarklÄ± model tiplerinin gÃ¼Ã§lÃ¼ yanlarÄ±nÄ± birleÅŸtirir
- **Meta-learner**: Genelde basit model (Ridge, Linear)
- **Cross-validation**: Base model tahminlerini oluÅŸturmak iÃ§in

#### Blending
- **TanÄ±m**: Holdout set ile meta-model eÄŸitimi
- **Fark**: Stacking'den daha basit, daha az CV gerektirir
- **Avantajlar**: HÄ±zlÄ± uygulama, overfitting riski dÃ¼ÅŸÃ¼k
- **KullanÄ±m**: BÃ¼yÃ¼k veri setlerinde

### 4. Model Diagnostics

#### Learning Curves
- **Training Curve**: Training set boyutuna gÃ¶re performans
- **Validation Curve**: Validation performansÄ± karÅŸÄ±laÅŸtÄ±rmasÄ±
- **Overfitting Detection**: Training-validation gap analizi
- **Underfitting Detection**: Her iki curve'Ã¼n de kÃ¶tÃ¼ performansÄ±

#### Validation Curves
- **TanÄ±m**: Tek parametrenin farklÄ± deÄŸerlerinde performans
- **Bias-Variance Trade-off**: Optimal parametre noktasÄ±
- **Model Complexity**: Parametrenin model karmaÅŸÄ±klÄ±ÄŸÄ±na etkisi

#### Performance Metrics
- **Regression**: RMSE, MAE, RÂ², MAPE
- **Classification**: Accuracy, Precision, Recall, F1-Score, AUC
- **Cross-validation**: Mean Â± Std deviation

### 5. Model Versioning ve Tracking

#### MLflow Components
- **Tracking**: Experiment ve run yÃ¶netimi
- **Projects**: Reproducible runs
- **Models**: Model packaging ve deployment
- **Registry**: Model versioning ve lifecycle

#### Experiment Tracking
- **Parameters**: Model hyperparameters
- **Metrics**: Performance metrics
- **Artifacts**: Model files, plots, data
- **Tags**: Metadata ve organizasyon

#### Model Registry
- **Versioning**: Model versiyonlarÄ± takibi
- **Stage Management**: None, Staging, Production, Archived
- **Model Lineage**: Model geliÅŸim geÃ§miÅŸi
- **Collaboration**: Team Ã§alÄ±ÅŸmasÄ± desteÄŸi

### 6. Advanced Training Strategies

#### Regularization
- **L1 (Lasso)**: Feature selection, sparse models
- **L2 (Ridge)**: Weight shrinkage, smooth models
- **Elastic Net**: L1 + L2 kombinasyonu
- **Dropout**: Neural network regularization

#### Early Stopping
- **TanÄ±m**: Validation loss artmaya baÅŸladÄ±ÄŸÄ±nda eÄŸitimi durdurma
- **Patience**: KaÃ§ epoch beklenecek
- **Restore Best**: En iyi model aÄŸÄ±rlÄ±klarÄ±nÄ± geri yÃ¼kleme
- **Monitoring**: Validation metric izleme

#### Data Augmentation
- **Image**: Rotation, flip, crop, noise
- **Text**: Synonym replacement, back-translation
- **Time Series**: Jittering, scaling, warping
- **Tabular**: SMOTE, noise injection

### 7. Performance Optimization

#### Feature Engineering
- **Selection**: Ã–nemli feature'larÄ± seÃ§me
- **Creation**: Yeni feature'lar tÃ¼retme
- **Transformation**: Scaling, encoding
- **Interaction**: Feature kombinasyonlarÄ±

#### Model Selection
- **Complexity**: Model karmaÅŸÄ±klÄ±ÄŸÄ± vs performance
- **Interpretability**: AÃ§Ä±klanabilirlik gereksinimleri
- **Speed**: Training ve inference hÄ±zÄ±
- **Memory**: Model boyutu ve RAM kullanÄ±mÄ±

#### Pipeline Optimization
- **Preprocessing**: Efficient data transformation
- **Parallel Processing**: Multi-core kullanÄ±mÄ±
- **Memory Management**: RAM optimizasyonu
- **Caching**: Intermediate results kaydetme

### 8. Best Practices

#### Experiment Design
- **Baseline Models**: Basit modeller ile baÅŸlama
- **Incremental Improvement**: AdÄ±m adÄ±m geliÅŸtirme
- **Ablation Studies**: Her deÄŸiÅŸikliÄŸin etkisini Ã¶lÃ§me
- **Reproducibility**: SonuÃ§larÄ± tekrarlanabilir kÄ±lma

#### Model Validation
- **Multiple Metrics**: Tek metric'e baÄŸÄ±mlÄ± kalmama
- **Statistical Significance**: T-test, confidence intervals
- **Business Metrics**: Technical metrics'in business impact'i
- **Robustness**: FarklÄ± veri setlerinde test

#### Documentation
- **Experiment Logs**: DetaylÄ± kayÄ±t tutma
- **Model Cards**: Model Ã¶zellikleri dokÃ¼mantasyonu
- **Code Comments**: Kodun aÃ§Ä±klanmasÄ±
- **Decision Rationale**: Neden bu yaklaÅŸÄ±m seÃ§ildi

## ğŸ” Ã–nemli Notlar

### Overfitting vs Underfitting
- **Overfitting**: Training'de iyi, validation'da kÃ¶tÃ¼
- **Underfitting**: Her ikisinde de kÃ¶tÃ¼ performans
- **Sweet Spot**: Optimal bias-variance trade-off

### Model Complexity
- **Simple Models**: HÄ±zlÄ±, yorumlanabilir, az veri
- **Complex Models**: YÃ¼ksek performans, Ã§ok veri gerektirir
- **Ensemble**: Complexity ve performance dengesinde

### Hyperparameter vs Parameter
- **Parameters**: Model tarafÄ±ndan Ã¶ÄŸrenilen (weights, biases)
- **Hyperparameters**: KullanÄ±cÄ± tarafÄ±ndan belirlenen (learning rate, depth)
- **Meta-parameters**: Hyperparameter optimization'un parametreleri

## ğŸš€ Advanced Topics

### AutoML
- **TanÄ±m**: Makine Ã¶ÄŸrenmesi pipeline'Ä±nÄ±n otomasyonu
- **Tools**: AutoSklearn, TPOT, H2O AutoML
- **Avantajlar**: HÄ±zlÄ± prototyping, domain expertise gerektirmez
- **SÄ±nÄ±rlar**: Esnek olmayan, black-box yaklaÅŸÄ±m

### Multi-objective Optimization
- **TanÄ±m**: Birden fazla metric'i aynÄ± anda optimize etme
- **Pareto Front**: Trade-off'lar arasÄ±nda optimal noktalar
- **NSGA-II**: Multi-objective genetic algorithm
- **Applications**: Accuracy vs speed, performance vs interpretability

### Transfer Learning
- **TanÄ±m**: Ã–nceden eÄŸitilmiÅŸ modeli yeni problem iÃ§in kullanma
- **Fine-tuning**: Son katmanlarÄ± yeniden eÄŸitme
- **Feature Extraction**: Ã–nceki katmanlarÄ± feature extractor olarak kullanma
- **Domain Adaptation**: FarklÄ± domain'lere uyarlama 